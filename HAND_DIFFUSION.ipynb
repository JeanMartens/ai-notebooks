{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc870b54-9451-4c60-8684-c12d753bce61",
   "metadata": {},
   "source": [
    "# Implementing the original Diffusion model for the HAND-MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ba2a0-50bd-44b1-8352-c1a1aedef74d",
   "metadata": {},
   "source": [
    "## Boring imports and datasets (Same for most of the Notebooks that use MNIST or HANDMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf43579-19c7-4e60-9cf6-4e7c81dbaa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn.model_selection as ms\n",
    "import math\n",
    "from tqdm import tqdm,trange\n",
    "import albumentations as A\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ef0156-8495-4230-b5b7-aa5d6bb2e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseNormalize(transforms.Normalize):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.tensor(mean).view(3,1,1)\n",
    "        self.std = torch.tensor(std).view(3,1,1)\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        tensor = (tensor * self.std.to(tensor.device)) + self.mean.to(tensor.device)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bea9527f-ddd2-48e0-9d01-01a6e627893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "    num_epochs = 20\n",
    "    lr = 1e-4\n",
    "    batch_size_train = 64\n",
    "    batch_size_valid = 64\n",
    "    num_latent_features = 100\n",
    "    embedding_dim = 256\n",
    "\n",
    "    image_size = (64,64,3)\n",
    "\n",
    "    T = 5\n",
    "    \n",
    "    discriminator_steps = 5\n",
    "    grad_penalty_lambda = 10 \n",
    "    \n",
    "    normalise_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=(0.5), std=(0.5))\n",
    "        ])\n",
    "\n",
    "    inverse_normalise_transform = InverseNormalize(mean=[227.8477, 229.4812, 222.2282], std=[21.2764, 14.5848, 29.2370])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9247f3-3634-4eb0-a923-431cf2f0a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Dataset(Dataset):\n",
    "    def __init__(self, metadata_df, images, normalise_transform = Hyperparams.normalise_transform ):\n",
    "        self.metadata_df = metadata_df\n",
    "        self.images = images\n",
    "        self.normalise_transform = normalise_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        idx = int(idx)\n",
    "\n",
    "        image = torch.tensor(images[idx]).unsqueeze(0)\n",
    "        label = torch.tensor([1])\n",
    "\n",
    "        if self.normalise_transform:\n",
    "            image = self.normalise_transform(image.float())\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6f25da9-5832-4651-bfb2-d7f80cf7a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('data/HANDMNIST/metadata.csv')\n",
    "train_metadata, valid_metadata = ms.train_test_split(metadata, test_size=0.2, train_size=0.8, random_state=19, shuffle=True, stratify=metadata['label'])\n",
    "\n",
    "images = np.load('data/HANDMNIST/images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f942f2d5-b7ba-48db-a247-3679177b1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST_Dataset(train_metadata.reset_index(), images[train_metadata.index])\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=Hyperparams.batch_size_train)\n",
    "\n",
    "valid_dataset = MNIST_Dataset(valid_metadata.reset_index(), images[valid_metadata.index])\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=Hyperparams.batch_size_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aab089-9e4f-4931-b9d5-e6ddb08c2e0a",
   "metadata": {},
   "source": [
    "## Fun part: models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8dd95c52-c403-4074-8d1d-000e27c782d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, residual = False):\n",
    "        super(DoubleConv, self).__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "        \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False), \n",
    "            nn.GroupNorm(1,out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False), \n",
    "            nn.GroupNorm(1,out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ) \n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1 = self.double_conv(x)\n",
    "        if self.residual:\n",
    "            return x1 + x\n",
    "        else:\n",
    "            return x1\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, embedding_dim = Hyperparams.embedding_dim):\n",
    "        super(Down, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        self.down_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(input_channels, input_channels, residual = True),\n",
    "            DoubleConv(input_channels, output_channels, residual = False),\n",
    "        )\n",
    "\n",
    "        self.embedding_layer = nn.Sequential(\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(embedding_dim, output_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self,x , t):\n",
    "        x = self.down_conv(x)\n",
    "        new_embedding = self.embedding_layer(t).view(t.shape[0],self.output_channels,1,1).repeat(1,1,x.shape[2], x.shape[3])\n",
    "        return x + new_embedding\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, embedding_dim = Hyperparams.embedding_dim):\n",
    "        super(Up, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        self.up_conv = nn.Sequential(\n",
    "            DoubleConv(input_channels, input_channels, residual = True),\n",
    "            DoubleConv(input_channels, output_channels, residual = False),\n",
    "            nn.ConvTranspose2d(output_channels, output_channels, kernel_size=4, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        self.embedding_layer = nn.Sequential(\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(embedding_dim, output_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self,x, from_skip_x,t):\n",
    "        x = torch.cat([x, from_skip_x], dim = 1)\n",
    "        x = self.up_conv(x)\n",
    "        new_embedding = self.embedding_layer(t).view(t.shape[0],self.output_channels,1,1).repeat(1,1,x.shape[2], x.shape[3])\n",
    "        return x + new_embedding\n",
    "\n",
    "    \n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, img_size):\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.down1 = Down(input_channels, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        self.down4 = Down(256, 512)\n",
    "\n",
    "        self.latent = DoubleConv(512,512)\n",
    "\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, output_channels)\n",
    "\n",
    "    def pos_encoding(self,t, embedding_dim = Hyperparams.embedding_dim):\n",
    "        \n",
    "        inv_freq = 1 / (10_000 ** torch.arange(0,embedding_dim,2) / embedding_dim).float()\n",
    "\n",
    "        pos_encoding_a = torch.sin(t * inv_freq)\n",
    "        pos_encoding_b = torch.cos(t * inv_freq)\n",
    "\n",
    "        pos_encoding = torch.cat([pos_encoding_a, pos_encoding_b], 0)\n",
    "        return pos_encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self,x, t):\n",
    "        t = self.pos_encoding(t)\n",
    "        \n",
    "        x1 = self.down1(x, t)\n",
    "        x2 = self.down2(x1, t)\n",
    "        x3 = self.down3(x2, t)\n",
    "        x4 = self.down4(x3, t)\n",
    "\n",
    "        latent = self.latent(x4)\n",
    "        \n",
    "        x5 = self.up1(latent,x4,t)\n",
    "        x6 = self.up2(x5,x3,t)\n",
    "        x7 = self.up3(x6,x2,t)\n",
    "        x8 = self.up4(x7,x1,t)\n",
    "\n",
    "        return x8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd049302-1866-4823-97bb-d8cee7d59bc1",
   "metadata": {},
   "source": [
    "## Some necessary functions for diffusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aee69be1-0dca-42d7-901a-c129fadca6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self,img_size, num_channels = 3, beta_start = 1e-4 , beta_end = 2e-2, T = 1000):\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.T = T\n",
    "        self.img_size = img_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "\n",
    "        self.betas = self.make_betas()\n",
    "        self.alphas = self.make_alphas()\n",
    "        self.alpha_bars = self.make_alpha_bars()\n",
    "        \n",
    "\n",
    "    def make_betas(self):\n",
    "        return torch.linspace(self.beta_start,self.beta_end,self.T)\n",
    "\n",
    "    def make_alphas(self):\n",
    "        return 1 - self.make_betas()\n",
    "\n",
    "    def make_alpha_bars(self):\n",
    "        return torch.cumprod(self.make_alphas(), 0)\n",
    "\n",
    "    def add_noise_to_img(self, image, t):\n",
    "        alpha_bar = self.alpha_bars[t]\n",
    "\n",
    "        sqrt_alpha_bar = torch.sqrt(alpha_bar).view(-1,1,1,1).repeat(1,1,image.shape[2],image.shape[3])\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar).view(-1,1,1,1).repeat(1,1,image.shape[2],image.shape[3])\n",
    "        noise = torch.randn(image.shape)\n",
    "\n",
    "        noised_image = sqrt_alpha_bar * image + sqrt_one_minus_alpha_bar * noise\n",
    "        return noised_image, noise\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(0, self.T, (n,))\n",
    "\n",
    "    def sample(self, model, fixed_noise, n): #sampling n new images for inference time\n",
    "        with torch.no_grad():\n",
    "            if fixed_noise == None:\n",
    "                noise_to_clear = torch.randn((n, self.num_channels, self.img_size, self.img_size)).to(device)\n",
    "            else:\n",
    "                noise_to_clear = fixed_noise\n",
    "\n",
    "            for t in range(self.T-1, -1, -1):\n",
    "                t_tensor = torch.tensor([t])\n",
    "                predicted_noise =  model(noise_to_clear, t_tensor)\n",
    "\n",
    "                beta = self.betas[t]\n",
    "                alpha = self.alphas[t]\n",
    "                alpha_bar = self.alpha_bars[t]\n",
    "\n",
    "                if t != 0:\n",
    "                    helping_noise = torch.randn_like(noise_to_clear)\n",
    "                else:\n",
    "                    helping_noise = torch.zeros_like(noise_to_clear)\n",
    "\n",
    "                noise_to_clear = ((1 / torch.sqrt(alpha))  * (noise_to_clear - ((beta / torch.sqrt(1 - alpha_bar)) * predicted_noise))) + torch.sqrt(beta) * helping_noise\n",
    "\n",
    "            new_images = inverse_normalise_transform(noise_to_clear)\n",
    "        return new_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ecabef0f-d0bf-4ff6-8d2e-735f104a873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, model, diffusion_tools, optimizer, criterion,device, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for index, data, in tqdm(enumerate(trainloader), total = len(train_loader)):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        batch_size = images.shape[0]\n",
    "        timesteps = diffusion_tools.sample_timesteps(batch_size)\n",
    "\n",
    "        noised_images, noise = diffusion_tools.add_noise_to_img(images, timesteps)\n",
    "        predicted_noise = model(noised_images,timesteps)\n",
    "\n",
    "        loss = criterion(predicted_noise, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a0edc29-07ab-4d57-a9e7-d776655379a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, fixed_input_noise, diffusion_tools, device, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_images = diffusion_tools.sample(model, fixed_input_noise, fixed_input_noise.shape[0])\n",
    "        generated_images = generated_images.cpu().numpy()\n",
    "\n",
    "        batch_size = generated_images.shape[0]\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            plt.subplot(1, batch_size, i + 1)\n",
    "            image = generated_images[i]\n",
    "            \n",
    "            if image.shape[0] > 1:\n",
    "                image = image.transpose(1, 2, 0)\n",
    "                \n",
    "            plt.imshow(image, cmap='gray' if image.shape[-1] != 3 else None)\n",
    "            plt.axis('off')\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "112ec732-afa5-4613-b730-865a0ae7eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "diffusion_tools = Diffusion(\n",
    "    img_size = Hyperparams.image_size[0], \n",
    "    num_channels = Hyperparams.image_size[2],\n",
    "    beta_start = 1e-4 , \n",
    "    beta_end = 2e-2, \n",
    "    T = Hyperparams.T)\n",
    "\n",
    "model = Unet(\n",
    "    input_channels = Hyperparams.image_size[2], \n",
    "    output_channels = Hyperparams.image_size[2], \n",
    "    img_size= Hyperparams.image_size[0]\n",
    ").to(device)\n",
    "\n",
    "fixed_input_noise = torch.randn((5,3,28,28)).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=Hyperparams.lr, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed08aaaa-c02e-4baf-a815-fc3552c0ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/433 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (128) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Hyperparams\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHyperparams\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_tools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     validate(model, fixed_input_noise, diffusion_tools, device, epoch)\n",
      "Cell \u001b[0;32mIn[66], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trainloader, model, diffusion_tools, optimizer, criterion, device, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m diffusion_tools\u001b[38;5;241m.\u001b[39msample_timesteps(batch_size)\n\u001b[1;32m     11\u001b[0m noised_images, noise \u001b[38;5;241m=\u001b[39m diffusion_tools\u001b[38;5;241m.\u001b[39madd_noise_to_img(images, timesteps)\n\u001b[0;32m---> 12\u001b[0m predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoised_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predicted_noise, noise)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[52], line 105\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x, t):\n\u001b[0;32m--> 105\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1(x, t)\n\u001b[1;32m    108\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(x1, t)\n",
      "Cell \u001b[0;32mIn[52], line 98\u001b[0m, in \u001b[0;36mUnet.pos_encoding\u001b[0;34m(self, t, embedding_dim)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_encoding\u001b[39m(\u001b[38;5;28mself\u001b[39m,t, embedding_dim \u001b[38;5;241m=\u001b[39m Hyperparams\u001b[38;5;241m.\u001b[39membedding_dim):\n\u001b[1;32m     96\u001b[0m     inv_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m10_000\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,embedding_dim,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m embedding_dim)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 98\u001b[0m     pos_encoding_a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minv_freq\u001b[49m)\n\u001b[1;32m     99\u001b[0m     pos_encoding_b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(t \u001b[38;5;241m*\u001b[39m inv_freq)\n\u001b[1;32m    101\u001b[0m     pos_encoding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([pos_encoding_a, pos_encoding_b], \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (128) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "for epoch in range(Hyperparams.num_epochs):\n",
    "    print(f'epoch {epoch}/{Hyperparams.num_epochs}')\n",
    "    train(train_loader, model, diffusion_tools, optimizer, criterion,device, epoch)\n",
    "    validate(model, fixed_input_noise, diffusion_tools, device, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef67e1-cb00-4993-aa5b-8483ef9b3c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
